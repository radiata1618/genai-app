import streamlit as st
import os
import requests
import requests
import io
import time
import queue
import asyncio
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed, wait

if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# Import logic from local_collector (in same directory)
import local_collector
from dotenv import load_dotenv
import sys

# Add backend directory to path to allow importing crawlers
script_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.abspath(os.path.join(script_dir, '..'))
if backend_dir not in sys.path:
    sys.path.append(backend_dir)

try:
    from crawlers import McKinseyCrawler, BCGCrawler
except ImportError:
    st.error("Crawler module check failed. Make sure 'playwright' is installed and 'backend/crawlers' exists.")


# Page Config
st.set_page_config(page_title="PDF Collector GUI", layout="wide")

# Title
st.title("ğŸ“š PDF Collection Tool (Local App)")
st.markdown("ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã‹ã‚‰ç›´æ¥ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€PDFã‚’åé›†ã—ã¦GCSã«ä¿å­˜ã—ã¾ã™ã€‚")

# --- Setup Environment & Auth (reusing logic) ---
# Load .env.local
script_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(script_dir, '../../.env.local')
load_dotenv(env_path)

# Auth fix
key_path = os.path.abspath(os.path.join(script_dir, '../../key.json'))
if os.path.exists(key_path):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = key_path

bucket_name = os.environ.get("GCS_BUCKET_NAME_FOR_CONSUL_DOC")

if not bucket_name:
    st.error("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° `GCS_BUCKET_NAME_FOR_CONSUL_DOC` ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`.env.local`ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- UI Components ---

# Input Type
input_type = st.radio("å…¥åŠ›ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ", ["URL (ãƒªãƒ³ã‚¯é›†PDF)", "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰)", "Google Search (Web)", "Site Crawler (Direct)"])

target_input = None
uploaded_file = None

if input_type.startswith("URL"):
    target_input = st.text_area("åé›†å¯¾è±¡ã®URL (è¤‡æ•°å¯: æ”¹è¡ŒåŒºåˆ‡ã‚Š)", placeholder="https://example.com/report1.pdf\nhttps://example.com/report2.pdf", height=150)
elif input_type.startswith("ãƒ­ãƒ¼ã‚«ãƒ«"):
    uploaded_files = st.file_uploader("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (è¤‡æ•°å¯)", type=["pdf"], accept_multiple_files=True)

# Google Search Config UI
CONSULTING_DOMAINS = {
    "McKinsey": "mckinsey.com",
    "BCG": "bcg.com",
    "Bain": "bain.com",
    "Deloitte": "deloitte.com",
    "PwC": "pwc.com",
    "EY": "ey.com",
    "Accenture": "accenture.com",
}
target_keywords = ""
target_domains = []
if input_type == "Google Search (Web)":
    st.info("Googleæ¤œç´¢ã‚’åˆ©ç”¨ã—ã¦PDFã‚’åé›†ã—ã¾ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨å¯¾è±¡ãƒ•ã‚¡ãƒ¼ãƒ ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    col1, col2 = st.columns([1, 1])
    with col1:
        target_keywords = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (ä¾‹: AI, DX, ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£)", placeholder="AI, DX")
    with col2:
        selected_firms = st.multiselect("å¯¾è±¡ãƒ•ã‚¡ãƒ¼ãƒ  (ãƒ‰ãƒ¡ã‚¤ãƒ³)", list(CONSULTING_DOMAINS.keys()), default=["McKinsey", "BCG", "Bain"])
        target_domains = [CONSULTING_DOMAINS[f] for f in selected_firms]

# Crawler Config UI
crawler_target_firms = []
crawler_limit = 10
crawler_regions = []
if input_type == "Site Crawler (Direct)":
    st.info("å„ç¤¾ã®ã‚µã‚¤ãƒˆã‚’ç›´æ¥å·¡å›ã—ã¦PDFã‚’åé›†ã—ã¾ã™ã€‚å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ãŒã€ç¶²ç¾…æ€§ãŒé«˜ã„ã§ã™ã€‚")
    col1, col2, col3 = st.columns([1, 1, 1])
    with col1:
       crawler_target_firms = st.multiselect("å·¡å›å¯¾è±¡", ["McKinsey", "BCG"], default=["McKinsey"])
    with col2:
       crawler_regions = st.multiselect("å¯¾è±¡ãƒªãƒ¼ã‚¸ãƒ§ãƒ³", ["Global (EN)", "Japan (JP)"], default=["Global (EN)"])
    with col3:
       crawler_limit = st.number_input("1ç¤¾/åœ°åŸŸã‚ãŸã‚Šã®æœ€å¤§æ•°", min_value=1, max_value=500, value=10)

# Run Button
if st.button("å®Ÿè¡Œ (Collect)", type="primary"):
    if input_type.startswith("URL") and not target_input.strip():
        st.warning("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    elif input_type.startswith("ãƒ­ãƒ¼ã‚«ãƒ«") and not uploaded_files:
        st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    elif input_type == "Google Search (Web)" and (not target_keywords or not target_domains):
        st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨å¯¾è±¡ãƒ•ã‚¡ãƒ¼ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    elif input_type == "Site Crawler (Direct)" and (not crawler_target_firms or not crawler_regions):
         st.warning("å·¡å›å¯¾è±¡ã¨ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    else:
        # --- Execution Logic ---
        st.divider()
        st.subheader("å®Ÿè¡Œãƒ­ã‚°")
        
        # Log container
        log_container = st.container()
        logs = []
        log_placeholder = log_container.empty()

        def gui_log(msg):
            ts = time.strftime("%H:%M:%S")
            logs.append(f"[{ts}] {msg}")
            log_placeholder.code("\n".join(logs))

        try:
            # 1. Prepare Targets
            all_targets = [] # List of (url, suggested_name)
            
            # --- LOCAL FILE MODE ---
            if input_type.startswith("ãƒ­ãƒ¼ã‚«ãƒ«"):
                input_sources = []
                for f in uploaded_files:
                    input_sources.append((f.name, f.read()))
                
                # Analyze each local file
                for idx, (source_name, content_bytes) in enumerate(input_sources):
                    gui_log(f"--- è¦ªPDFå‡¦ç† ({idx+1}/{len(input_sources)}): {source_name} ---")
                    pdf_stream_ai = io.BytesIO(content_bytes)
                    pdf_stream_links = io.BytesIO(content_bytes)
                    
                    gui_log(f"[*] AIè§£æã‚’å®Ÿè¡Œä¸­...")
                    gemini_metadata = local_collector.scan_pdf_with_gemini(pdf_stream_ai, log_func=gui_log)
                    gui_log(f"[*] ãƒªãƒ³ã‚¯æŠ½å‡ºã‚’å®Ÿè¡Œä¸­...")
                    raw_links = local_collector.extract_pdf_links(pdf_stream_links, log_func=gui_log)
                    
                    # Merge
                    for link in raw_links:
                        suggested = None
                        for meta_url, meta_name in gemini_metadata.items():
                            if meta_url in link or link in meta_url:
                                suggested = meta_name
                                break
                        all_targets.append((link, suggested))
                    gui_log(f"[+] {len(raw_links)} ä»¶ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã—ãŸã€‚")

            # --- URL LIST MODE ---
            elif input_type.startswith("URL"):
                urls = [u.strip() for u in target_input.split('\n') if u.strip()]
                for i, url in enumerate(urls):
                    gui_log(f"[*] URLã‹ã‚‰è¦ªPDFã‚’å–å¾—ä¸­ ({i+1}/{len(urls)}): {url}")
                    try:
                        res = requests.get(url, headers=local_collector.HEADERS, timeout=60)
                        res.raise_for_status()
                        content_bytes = res.content
                        
                        # Same logic as Local File
                        pdf_stream_ai = io.BytesIO(content_bytes)
                        pdf_stream_links = io.BytesIO(content_bytes)
                        gemini_metadata = local_collector.scan_pdf_with_gemini(pdf_stream_ai, log_func=gui_log)
                        raw_links = local_collector.extract_pdf_links(pdf_stream_links, log_func=gui_log)
                        
                        for link in raw_links:
                             suggested = None
                             for meta_url, meta_name in gemini_metadata.items():
                                 if meta_url in link or link in meta_url:
                                     suggested = meta_name
                                     break
                             all_targets.append((link, suggested))
                             
                    except Exception as e:
                        gui_log(f"[ã‚¨ãƒ©ãƒ¼] URLå–å¾—å¤±æ•— ({url}): {e}")

            # --- GOOGLE SEARCH MODE ---
            elif input_type == "Google Search (Web)":
                api_key = os.environ.get("GOOGLE_SEARCH_API_KEY") or os.environ.get("GOOGLE_CLOUD_API_KEY")
                cx = os.environ.get("GOOGLE_SEARCH_CX")
                
                if not api_key or not cx:
                    st.error("ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    st.stop()

                gui_log(f"[*] Googleæ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™... ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {target_keywords}")
                site_query = " OR ".join([f"site:{d}" for d in target_domains])
                full_query = f"{target_keywords} filetype:pdf ({site_query})"
                
                results = local_collector.search_pdfs_via_google(api_key, cx, full_query, num_results=20, log_func=gui_log)
                for res in results:
                    title_clean = local_collector.clean_text(res['title'])
                    all_targets.append((res['link'], title_clean))
            
            # --- CRAWLER MODE ---
            elif input_type == "Site Crawler (Direct)":
                gui_log(f"[*] ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™ã€‚å¯¾è±¡: {crawler_target_firms}, Limit: {crawler_limit}")
                
                # Map UI selection to locale codes
                target_locales = []
                if "Global (EN)" in crawler_regions: target_locales.append("en")
                if "Japan (JP)" in crawler_regions: target_locales.append("jp")

                for firm in crawler_target_firms:
                    for locale in target_locales:
                        locale_label = "Global" if locale == "en" else "Japan"
                        gui_log(f"--- Crawling {firm} ({locale_label}) ---")
                        try:
                            crawler = None
                            if firm == "McKinsey":
                                crawler = McKinseyCrawler(log_func=gui_log)
                            elif firm == "BCG":
                                crawler = BCGCrawler(log_func=gui_log)
                            
                            if crawler:
                                results = crawler.crawl(limit=crawler_limit, locale=locale)
                                for res in results:
                                    title_clean = local_collector.clean_text(res['title'])
                                    # Append locale to title for clarity
                                    if locale == "jp":
                                        title_clean = f"[JP] {title_clean}"
                                    all_targets.append((res['url'], title_clean))
                        except Exception as e:
                            gui_log(f"[Error] {firm} ({locale}) crawler failed: {e}")


            # 2. Results Processing
            # Remove duplicates based on URL
            unique_targets = {}
            for t in all_targets:
                if t[0] not in unique_targets:
                    unique_targets[t[0]] = t
            all_targets = list(unique_targets.values())

            gui_log(f"==========================================")
            gui_log(f"[*] è§£æå®Œäº†ã€‚åˆè¨ˆ {len(all_targets)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
            
            if not all_targets:
                gui_log("[*] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                 # 3. Batch Download
                targets = all_targets
                gui_log(f"[*] ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ (ä¸¦åˆ—æ•°: 5)")
                
                progress_bar = st.progress(0)
                total_links = len(targets)
                log_queue = queue.Queue()
                def queue_logger(msg):
                    log_queue.put(msg)

                with ThreadPoolExecutor(max_workers=5) as ex:
                    futures = []
                    for t in targets:
                        futures.append(ex.submit(local_collector.download_and_upload, t[0], bucket_name, t[1], log_func=queue_logger))
                    
                    completed_count = 0
                    while True:
                        while not log_queue.empty():
                             gui_log(log_queue.get())
                        dones = sum(1 for f in futures if f.done())
                        if total_links > 0:
                            progress_bar.progress(min(dones / total_links, 1.0))
                        if dones == total_links:
                            break
                        time.sleep(0.5)
                    while not log_queue.empty():
                         gui_log(log_queue.get())

                gui_log("[*] å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                st.success("å®Œäº†ã—ã¾ã—ãŸï¼")

        except Exception as e:
            st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {type(e).__name__}: {e}")
            gui_log(f"[EXCEPTION] {repr(e)}")
            import traceback
            traceback.print_exc()
